{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "The basics of market basket analysis\n",
    "Market basket analysis uses lists of transactions to identify useful associations between items. Such associations can be written in the form of a rule that has an antecedent and a consequent. Let's assume a small grocery store has asked you to look at their transaction data. After some analysis, you find the rule given below.\n",
    "\n",
    "{cereal}--- {milk}\n",
    "\n",
    "\n",
    "{cereal} is the antecedent, {milk} is the consequent, and both are items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In [1]:\n",
    "groceries.head()\n",
    "Out[1]:\n",
    "\n",
    "                 Transaction\n",
    "0         milk,bread,biscuit\n",
    "1  bread,milk,biscuit,cereal\n",
    "2                  bread,tea\n",
    "3             jam,bread,milk\n",
    "4                tea,biscuit\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas under the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load transactions from pandas\n",
    "groceries = pd.read_csv(groceries_path)\n",
    "\n",
    "# Split transaction strings into lists\n",
    "transactions = groceries['Transaction'].apply(lambda t: t.split(','))\n",
    "\n",
    "# Convert DataFrame column into list of strings\n",
    "transactions = list(transactions)\n",
    "\n",
    "# Print the list of transactions\n",
    "print(transactions)\n",
    "'''\n",
    "[['milk', 'bread', 'biscuit'], ['bread', 'milk', 'biscuit', 'cereal'], ['bread', 'tea'], ['jam', 'bread', 'milk'], ['tea', 'biscuit'], ['bread', 'tea'], ['tea', 'cereal'], ['bread', 'tea', 'biscuit'], ['jam', 'bread', 'tea'], ['bread', 'milk'], ['coffee', 'orange', 'biscuit', 'cereal'], ['coffee', 'orange', 'biscuit', 'cereal'], ['coffee', 'sugar'], ['bread', 'coffee', 'orange'], ['bread', 'sugar', 'biscuit'], ['coffee', 'sugar', 'cereal'], ['bread', 'sugar', 'biscuit'], ['bread', 'coffee', 'sugar'], ['bread', 'coffee', 'sugar'], ['tea', 'milk', 'coffee', 'cereal']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import permutations from the itertools module\n",
    "from itertools import permutations\n",
    "\n",
    "# Define the set of groceries\n",
    "flattened = [i for t in transactions for i in t]\n",
    "groceries = list(set(flattened))\n",
    "\n",
    "# Generate all possible rules\n",
    "rules = list(permutations(groceries, 2))\n",
    "\n",
    "# Print the set of rules\n",
    "print(rules)\n",
    "\n",
    "# Print the number of rules\n",
    "print(len(rules))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[('cereal', 'biscuit'), ('cereal', 'milk'), ('cereal', 'orange'), ('cereal', 'coffee'), ('cereal', 'bread'), ('cereal', 'jam'), ('cereal', 'sugar'), ('cereal', 'tea'), ('biscuit', 'cereal'), ('biscuit', 'milk'), ('biscuit', 'orange'), ('biscuit', 'coffee'), ('biscuit', 'bread'), ('biscuit', 'jam'), ('biscuit', 'sugar'), ('biscuit', 'tea'), ('milk', 'cereal'), ('milk', 'biscuit'), ('milk', 'orange'), ('milk', 'coffee'), ('milk', 'bread'), ('milk', 'jam'), ('milk', 'sugar'), ('milk', 'tea'), ('orange', 'cereal'), ('orange', 'biscuit'), ('orange', 'milk'), ('orange', 'coffee'), ('orange', 'bread'), ('orange', 'jam'), ('orange', 'sugar'), ('orange', 'tea'), ('coffee', 'cereal'), ('coffee', 'biscuit'), ('coffee', 'milk'), ('coffee', 'orange'), ('coffee', 'bread'), ('coffee', 'jam'), ('coffee', 'sugar'), ('coffee', 'tea'), ('bread', 'cereal'), ('bread', 'biscuit'), ('bread', 'milk'), ('bread', 'orange'), ('bread', 'coffee'), ('bread', 'jam'), ('bread', 'sugar'), ('bread', 'tea'), ('jam', 'cereal'), ('jam', 'biscuit'), ('jam', 'milk'), ('jam', 'orange'), ('jam', 'coffee'), ('jam', 'bread'), ('jam', 'sugar'), ('jam', 'tea'), ('sugar', 'cereal'), ('sugar', 'biscuit'), ('sugar', 'milk'), ('sugar', 'orange'), ('sugar', 'coffee'), ('sugar', 'bread'), ('sugar', 'jam'), ('sugar', 'tea'), ('tea', 'cereal'), ('tea', 'biscuit'), ('tea', 'milk'), ('tea', 'orange'), ('tea', 'coffee'), ('tea', 'bread'), ('tea', 'jam'), ('tea', 'sugar')]\n",
    "72\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae072fd1",
   "metadata": {},
   "source": [
    "## Support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c425604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "transactions\n",
    "\n",
    "\n",
    "[['milk', 'bread', 'biscuit'],\n",
    " ['bread', 'milk', 'biscuit', 'cereal'],\n",
    " ['bread', 'tea'],\n",
    " ['jam', 'bread', 'milk'],\n",
    " ['tea', 'biscuit'],\n",
    " ['bread', 'tea'],\n",
    " ['tea', 'cereal'],\n",
    " ['bread', 'tea', 'biscuit'],\n",
    " ['jam', 'bread', 'tea'],\n",
    " ['bread', 'milk'],\n",
    " ['coffee', 'orange', 'biscuit', 'cereal'],\n",
    " ['coffee', 'orange', 'biscuit', 'cereal'],\n",
    " ['coffee', 'sugar'],\n",
    " ['bread', 'coffee', 'orange'],\n",
    " ['bread', 'sugar', 'biscuit'],\n",
    " ['coffee', 'sugar', 'cereal'],\n",
    " ['bread', 'sugar', 'biscuit'],\n",
    " ['bread', 'coffee', 'sugar'],\n",
    " ['bread', 'coffee', 'sugar'],\n",
    " ['tea', 'milk', 'coffee', 'cereal']]\n",
    "\"\"\"\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4bcd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the transaction encoder function from mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Instantiate transaction encoder and identify unique items\n",
    "encoder = TransactionEncoder().fit(transactions)\n",
    "\n",
    "# One-hot encode transactions\n",
    "onehot = encoder.transform(transactions)\n",
    "\n",
    "# Convert one-hot encoded data to DataFrame\n",
    "onehot = pd.DataFrame(onehot, columns = encoder.columns_)\n",
    "\n",
    "# Print the one-hot encoded transaction dataset\n",
    "print(onehot)\n",
    "\"\"\"\n",
    "In [5]:\n",
    "onehot\n",
    "Out[5]:\n",
    "\n",
    "    biscuit  bread  cereal  coffee    jam   milk  orange  sugar    tea\n",
    "0      True   True   False   False  False   True   False  False  False\n",
    "1      True   True    True   False  False   True   False  False  False\n",
    "2     False   True   False   False  False  False   False  False   True\n",
    "3     False   True   False   False   True   True   False  False  False\n",
    "4      True  False   False   False  False  False   False  False   True\n",
    "5     False   True   False   False  False  False   False  False   True\n",
    "6     False  False    True   False  False  False   False  False   True\n",
    "7      True   True   False   False  False  False   False  False   True\n",
    "8     False   True   False   False   True  False   False  False   True\n",
    "9     False   True   False   False  False   True   False  False  False\n",
    "10     True  False    True    True  False  False    True  False  False\n",
    "11     True  False    True    True  False  False    True  False  False\n",
    "12    False  False   False    True  False  False   False   True  False\n",
    "13    False   True   False    True  False  False    True  False  False\n",
    "14     True   True   False   False  False  False   False   True  False\n",
    "15    False  False    True    True  False  False   False   True  False\n",
    "16     True   True   False   False  False  False   False   True  False\n",
    "17    False   True   False    True  False  False   False   True  False\n",
    "18    False   True   False    True  False  False   False   True  False\n",
    "19    False  False    True    True  False   True   False  False   True\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Compute the support\n",
    "support = onehot.mean()\n",
    "\n",
    "# Print the support\n",
    "print(support)\n",
    "\"\"\"\n",
    "biscuit    0.40\n",
    "bread      0.65\n",
    "cereal     0.30\n",
    "coffee     0.40\n",
    "jam        0.10\n",
    "milk       0.25\n",
    "orange     0.15\n",
    "sugar      0.30\n",
    "tea        0.35\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574dbb1f",
   "metadata": {},
   "source": [
    "## How  logical_and works in numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program explaining\n",
    "# logical_and() function\n",
    "import numpy as np\n",
    "  \n",
    "# input\n",
    "arr1 = [1, 3, False, 4]\n",
    "arr2 = [3, 0, True, False]\n",
    "  \n",
    "# output\n",
    "out_arr = np.logical_and(arr1, arr2)\n",
    "  \n",
    "print (\"Output Array : \", out_arr)\n",
    "\"\"\"\n",
    "Output Array :  [ True False False False]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a47ddd",
   "metadata": {},
   "source": [
    "## Business Question:- In this exercise, you'll make use of that DataFrame and the support metric to help the store's owner. First, she has asked you to identify frequently purchased items, which you'll do by computing support at the item-level. And second, she asked you to check whether the rule {jam} --- {bread} has a support of over 0.05. Note that onehot has been defined and is available. Additionally, pandas has been imported under the alias pd and numpy has been imported under the alias np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdb2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a jam+bread column to the DataFrame onehot\n",
    "onehot['jam+bread'] = np.logical_and(onehot['jam'], (onehot['bread']))\n",
    "\n",
    "# Compute the support\n",
    "support = onehot.mean()\n",
    "\n",
    "# Print the support values\n",
    "print(support)\n",
    "\"\"\"\n",
    "# Print the support values\n",
    "print(support)\n",
    "biscuit      0.40\n",
    "bread        0.65\n",
    "cereal       0.30\n",
    "coffee       0.40\n",
    "jam          0.10\n",
    "milk         0.25\n",
    "orange       0.15\n",
    "sugar        0.30\n",
    "tea          0.35\n",
    "jam+bread    0.10\n",
    "dtype: float64\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb95d46",
   "metadata": {},
   "source": [
    "# Recommending books with support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45663461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "books \n",
    "\n",
    "  Hunger  Potter  Twilight\n",
    "0      False    True     False\n",
    "1      False    True      True\n",
    "2      False   False     False\n",
    "3      False    True     False\n",
    "4      False   False     False\n",
    "...      ...     ...       ...\n",
    "8045   False   False     False\n",
    "8046   False   False     False\n",
    "8047   False   False      True\n",
    "8048    True   False      True\n",
    "8049   False   False     False\n",
    "\n",
    "[8050 rows x 3 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c874912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute support for Hunger and Potter\n",
    "supportHP = np.logical_and(books['Hunger'], books['Potter']).mean()\n",
    "\n",
    "# Compute support for Hunger and Twilight\n",
    "supportHT = np.logical_and(books['Hunger'], books['Twilight']).mean()\n",
    "\n",
    "# Compute support for Potter and Twilight\n",
    "supportPT = np.logical_and(books['Potter'], books['Twilight']).mean()\n",
    "\n",
    "# Print support values\n",
    "print(\"Hunger Games and Harry Potter: %.2f\" % supportHP)\n",
    "print(\"Hunger Games and Twilight: %.2f\" % supportHT)\n",
    "print(\"Harry Potter and Twilight: %.2f\" % supportPT)\n",
    "\n",
    "\"\"\"\n",
    "Hunger Games and Harry Potter: 0.12\n",
    "Hunger Games and Twilight: 0.09\n",
    "Harry Potter and Twilight: 0.14(Max)\n",
    "\n",
    "------------- Inference -------------------\n",
    "Based on the support metric, \n",
    "Harry Potter and Twilight appear to be the best options for cross-promotion. \n",
    "In the next problem, we'll consider whether we should use Harry Potter to promote Twilight or Twilight to promote Harry Potter.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute support for Potter and Twilight\n",
    "supportPT = np.logical_and(books['Potter'], books['Twilight']).mean()\n",
    "\n",
    "# Compute support for Potter\n",
    "supportP = books['Potter'].mean()\n",
    "\n",
    "# Compute support for Twilight\n",
    "supportT = books['Twilight'].mean()\n",
    "\n",
    "# Compute confidence for both rules\n",
    "confidencePT = supportPT / supportP\n",
    "confidenceTP = supportPT / supportT\n",
    "\n",
    "# Print results\n",
    "print('{0:.2f}, {1:.2f}'.format(confidencePT, confidenceTP))\n",
    "\n",
    "\"\"\"\n",
    "confidencePT:0.29\n",
    "confidenceTP:0.55\n",
    "\n",
    "----- Inference ---------\n",
    "Even though the support is identical for the two association rules, \n",
    "the confidence is much higher for Twilight -> Harry Potter, \n",
    "since Harry Potter has a higher support than Twilight.\n",
    "SO Use Twilight to promote Harry Potter,\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute support for Potter and Twilight\n",
    "supportPT = np.logical_and(books['Potter'], books['Twilight']).mean()\n",
    "\n",
    "# Compute support for Potter\n",
    "supportP = books['Potter'].mean()\n",
    "\n",
    "# Compute support for Twilight\n",
    "supportT = books['Twilight'].mean()\n",
    "\n",
    "# Compute lift\n",
    "lift = supportPT / (supportP * supportT)\n",
    "\n",
    "# Print lift\n",
    "print(\"Lift: %.2f\" % lift)\n",
    "\n",
    "\"\"\"\n",
    "Lift: 1.15\n",
    "\n",
    "---- Inference------\n",
    "As it turns out, lift is greater than 1.0.Twilight to promote Harry Potter, \n",
    "This could give us some confidence that the association rule we recommended did not arise by random chance.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9bd6b5",
   "metadata": {},
   "source": [
    "## Conviction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df88c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conviction(antecedent, consequent):\n",
    "\t# Compute support for antecedent AND consequent\n",
    "\tsupportAC = np.logical_and(antecedent, consequent).mean()\n",
    "\n",
    "\t# Compute support for antecedent\n",
    "\tsupportA = antecedent.mean()\n",
    "\n",
    "\t# Compute support for NOT consequent\n",
    "\tsupportnC = 1.0 - consequent.mean()\n",
    "\n",
    "\t# Compute support for antecedent and NOT consequent\n",
    "\tsupportAnC = supportA - supportAC\n",
    "\n",
    "    # Return conviction\n",
    "\treturn supportA * supportnC / supportAn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51255eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conviction for twilight -> potter and potter -> twilight\n",
    "convictionTP = conviction(twilight, potter)\n",
    "convictionPT = conviction(potter, twilight)\n",
    "\n",
    "# Compute conviction for twilight -> hunger and hunger -> twilight\n",
    "convictionTH = conviction(twilight, hunger)\n",
    "convictionHT = conviction(hunger, twilight)\n",
    "\n",
    "# Compute conviction for potter -> hunger and hunger -> potter\n",
    "convictionPH = conviction(potter, hunger)\n",
    "convictionHP = conviction(hunger, potter)\n",
    "\n",
    "# Print results\n",
    "print('Harry Potter -> Twilight: ', convictionPT)\n",
    "print('Twilight -> Potter: ', convictionTP)\n",
    "'''\n",
    "Harry Potter -> Twilight:  1.0534570072738598\n",
    "Twilight -> Potter:  1.1550539077290998 \n",
    "Inference : Twilight to promote Harry Potter\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e7a64",
   "metadata": {},
   "source": [
    "# Zangs-Metric : is between -1 to 1 , -1 mean strong disassociation and 1 is strong association "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2471fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the support of Twilight and Harry Potter\n",
    "supportT = books['Twilight'].mean()\n",
    "supportP = books['Potter'].mean()\n",
    "\n",
    "# Compute the support of both books\n",
    "supportTP = np.logical_and(books['Twilight'], books['Potter']).mean()\n",
    "\n",
    "# Complete the expressions for the numerator and denominator\n",
    "numerator = supportTP - supportT*supportP\n",
    "denominator = max(supportTP*(1-supportT), supportT*(supportP-supportTP))\n",
    "\n",
    "# Compute and print Zhang's metric\n",
    "zhang = numerator / denominator\n",
    "print(zhang)\n",
    "'''\n",
    "0.17231567178855997\n",
    "`if Twilight then Harry Potter'' proved robust. \n",
    "It had a positive value for Zhang's metric, indicating that the two books are not dissociated.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute Zhang's metric\n",
    "def zhang(antecedent, consequent):\n",
    "\t# Compute the support of each book\n",
    "\tsupportA = antecedent.mean()\n",
    "\tsupportC = consequent.mean()\n",
    "\n",
    "\t# Compute the support of both books\n",
    "\tsupportAC = np.logical_and(antecedent, consequent).mean()\n",
    "\n",
    "\t# Complete the expressions for the numerator and denominator\n",
    "\tnumerator = supportAC - supportA*supportC\n",
    "\tdenominator = max(supportAC*(1-supportA), supportA*(supportC-supportAC))\n",
    "\n",
    "\t# Return Zhang's metric\n",
    "\treturn numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cb30f",
   "metadata": {},
   "source": [
    "## The founder of the ebook start-up has returned for additional consulting services. She has sent you a list of itemsets she's investigating and has asked you to determine whether any of them contain items that are dissociated. When you're finished, she has asked that you add the metric you use to a column in the rules DataFrame, which is available to you, and currently contains columns for antecedents and consequents.\n",
    "\n",
    "The itemsets are available as a list of lists called itemsets. Each list contains the antecedent first and the consequent second. You also have access to the books DataFrame from previous exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "books \n",
    "\n",
    "\n",
    "      Hunger  Potter  Twilight  Mockingbird  Gatsby\n",
    "0      False    True     False         True    True\n",
    "1      False    True      True        False    True\n",
    "2      False   False     False         True   False\n",
    "3      False    True     False        False    True\n",
    "4      False   False     False        False    True\n",
    "...      ...     ...       ...          ...     ...\n",
    "8045   False   False     False         True    True\n",
    "8046   False   False     False         True   False\n",
    "8047   False   False      True        False   False\n",
    "8048    True   False      True        False   False\n",
    "8049   False   False     False         True   False\n",
    "\n",
    "----------------------------------------------\n",
    "In [5]:\n",
    "itemsets\n",
    "Out[5]:\n",
    "\n",
    "[['Potter', 'Hunger'],\n",
    " ['Twilight', 'Hunger'],\n",
    " ['Mockingbird', 'Hunger'],\n",
    " ['Gatsby', 'Hunger'],\n",
    " ['Potter', 'Twilight'],\n",
    " ['Potter', 'Mockingbird'],\n",
    " ['Potter', 'Gatsby'],\n",
    " ['Mockingbird', 'Twilight'],\n",
    " ['Gatsby', 'Twilight'],\n",
    " ['Mockingbird', 'Gatsby']]\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b810fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list for Zhang's metric\n",
    "zhangs_metric = []\n",
    "\n",
    "# Loop over lists in itemsets\n",
    "for itemset in itemsets:\n",
    "    # Extract the antecedent and consequent columns\n",
    "\tantecedent = books[itemset[0]]\n",
    "\tconsequent = books[itemset[1]]\n",
    "    #print('antecedent',antecedent)\n",
    "\t#print('consequent',consequent)\n",
    "    # Complete Zhang's metric and append it to the list\n",
    "\tzhangs_metric.append(zhang(antecedent, consequent))\n",
    "    \n",
    "# Print results\n",
    "rules['zhang'] = zhangs_metric\n",
    "print(rules)\n",
    "\"\"\"\n",
    "\n",
    " antecedents  consequents     zhang\n",
    "    0       Potter       Hunger -0.306049\n",
    "    1     Twilight       Hunger  0.109357\n",
    "    2  Mockingbird       Hunger -0.525436\n",
    "    3       Gatsby       Hunger -0.550446\n",
    "    4       Potter     Twilight  0.245118\n",
    "    5       Potter  Mockingbird -0.065537\n",
    "    6       Potter       Gatsby -0.165572\n",
    "    7  Mockingbird     Twilight -0.319008\n",
    "    8       Gatsby     Twilight -0.370875\n",
    "    9  Mockingbird       Gatsby  0.466460\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8709d7",
   "metadata": {},
   "source": [
    "## Multi-Metric Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0144551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In [1]:\n",
    "rules\n",
    "Out[1]:\n",
    "\n",
    "          antecedents                      consequents  antecedent support  consequent support   support  confidence      lift  leverage  conviction\n",
    "0            (Hunger)                         (Potter)            0.319130            0.477516  0.123851    0.388089  0.812725 -0.028539    0.853857\n",
    "1            (Potter)                         (Hunger)            0.477516            0.319130  0.123851    0.259365  0.812725 -0.028539    0.919305\n",
    "2            (Hunger)                       (Twilight)            0.319130            0.256770  0.089193    0.279486  1.088468  0.007249    1.031527\n",
    "3          (Twilight)                         (Hunger)            0.256770            0.319130  0.089193    0.347363  1.088468  0.007249    1.043260\n",
    "4            (Hunger)                    (Mockingbird)            0.319130            0.476522  0.096273    0.301674  0.633075 -0.055799    0.749619\n",
    "..                ...                              ...                 ...                 ...       ...         ...       ...       ...         ...\n",
    "145  (Potter, Gatsby)          (Twilight, Mockingbird)            0.127702            0.098261  0.024348    0.190661  1.940360  0.011800    1.114168\n",
    "146        (Twilight)    (Potter, Mockingbird, Gatsby)            0.256770            0.089814  0.024348    0.094823  1.055779  0.001286    1.005535\n",
    "147     (Mockingbird)       (Twilight, Potter, Gatsby)            0.476522            0.034161  0.024348    0.051095  1.495687  0.008069    1.017845\n",
    "148          (Potter)  (Twilight, Mockingbird, Gatsby)            0.477516            0.036273  0.024348    0.050989  1.405678  0.007027    1.015506\n",
    "149          (Gatsby)  (Twilight, Potter, Mockingbird)            0.295155            0.062981  0.024348    0.082492  1.309778  0.005759    1.021264\n",
    "\n",
    "[150 rows x 9 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the rules DataFrame using the .head() method\n",
    "print(rules.head())\n",
    "\n",
    "# Select the subset of rules with antecedent support greater than 0.05\n",
    "rules = rules[rules['antecedent support'] > 0.05]\n",
    "\n",
    "# Select the subset of rules with a consequent support greater than 0.02\n",
    "rules = rules[rules['consequent support'] > 0.02]\n",
    "\n",
    "# Select the subset of rules with a conviction greater than 1.01\n",
    "rules = rules[rules['conviction'] > 1.01]\n",
    "\n",
    "# Print remaining rules\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ea210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " antecedents    consequents  antecedent support  consequent support   support  confidence      lift  leverage  conviction\n",
    "0    (Hunger)       (Potter)            0.319130            0.477516  0.123851    0.388089  0.812725 -0.028539    0.853857\n",
    "1    (Potter)       (Hunger)            0.477516            0.319130  0.123851    0.259365  0.812725 -0.028539    0.919305\n",
    "2    (Hunger)     (Twilight)            0.319130            0.256770  0.089193    0.279486  1.088468  0.007249    1.031527\n",
    "3  (Twilight)       (Hunger)            0.256770            0.319130  0.089193    0.347363  1.088468  0.007249    1.043260\n",
    "4    (Hunger)  (Mockingbird)            0.319130            0.476522  0.096273    0.301674  0.633075 -0.055799    0.749619\n",
    "               antecedents                      consequents  antecedent support  consequent support   support  confidence      lift  leverage  conviction\n",
    "2                 (Hunger)                       (Twilight)            0.319130            0.256770  0.089193    0.279486  1.088468  0.007249    1.031527\n",
    "3               (Twilight)                         (Hunger)            0.256770            0.319130  0.089193    0.347363  1.088468  0.007249    1.043260\n",
    "8               (Twilight)                         (Potter)            0.256770            0.477516  0.140621    0.547654  1.146881  0.018009    1.155054\n",
    "9                 (Potter)                       (Twilight)            0.477516            0.256770  0.140621    0.294485  1.146881  0.018009    1.053457\n",
    "18           (Mockingbird)                         (Gatsby)            0.476522            0.295155  0.186087    0.390511  1.323070  0.045439    1.156452\n",
    "..                     ...                              ...                 ...                 ...       ...         ...       ...       ...         ...\n",
    "143  (Potter, Mockingbird)               (Twilight, Gatsby)            0.219503            0.053540  0.024348    0.110922  2.071754  0.012596    1.064541\n",
    "145       (Potter, Gatsby)          (Twilight, Mockingbird)            0.127702            0.098261  0.024348    0.190661  1.940360  0.011800    1.114168\n",
    "147          (Mockingbird)       (Twilight, Potter, Gatsby)            0.476522            0.034161  0.024348    0.051095  1.495687  0.008069    1.017845\n",
    "148               (Potter)  (Twilight, Mockingbird, Gatsby)            0.477516            0.036273  0.024348    0.050989  1.405678  0.007027    1.015506\n",
    "149               (Gatsby)  (Twilight, Potter, Mockingbird)            0.295155            0.062981  0.024348    0.082492  1.309778  0.005759    1.021264\n",
    "\n",
    "[82 rows x 9 columns]\n",
    "\n",
    "\n",
    "\n",
    "------ Inference:\n",
    "ou have now successfully performed multi-metric filtering. \n",
    "In the final exercise in this chapter, you'll go even further by including an advanced metric.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the lift threshold to 1.5\n",
    "rules = rules[rules['lift'] > 1.5]\n",
    "\n",
    "# Set the conviction threshold to 1.0\n",
    "rules = rules[rules['conviction']>1]\n",
    "\n",
    "# Set the threshold for Zhang's rule to 0.65\n",
    "rules = rules[rules['zhang']>0.65]\n",
    "\n",
    "# Print rule\n",
    "print(rules[['antecedents','consequents']])\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "<script.py> output:\n",
    "                     antecedents               consequents\n",
    "    115    (Potter, Mockingbird)          (Hunger, Gatsby)\n",
    "    119            (Mockingbird)  (Hunger, Potter, Gatsby)\n",
    "    127    (Hunger, Mockingbird)        (Twilight, Gatsby)\n",
    "    129  (Twilight, Mockingbird)          (Hunger, Gatsby)\n",
    "    143    (Potter, Mockingbird)        (Twilight, Gatsby)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf284b",
   "metadata": {},
   "source": [
    "## Performing aggregation\n",
    "After completing minor consulting jobs for a library and an ebook seller, you've finally received your first big market basket analysis project: advising an online novelty gifts retailer on cross-promotions. Since the retailer has never previously hired a data scientist, it would like you to start the project by exploring its transaction data. It has asked you to perform aggregation for all signs in the dataset and also compute the support for this category. Note that pandas has been imported for you as pd. Additionally, the data has been imported in one-hot encoded format as onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeee1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "onehot\n",
    "\n",
    "      50'S CHRISTMAS GIFT BAG LARGE   DOLLY GIRL BEAKER   I LOVE LONDON MINI BACKPACK   RED SPOT GIFT BAG LARGE   SPACEBOY BABY GIFT SET  12 MESSAGE CARDS WITH ENVELOPES  12 PENCIL SMALL TUBE WOODLAND  ...  ZINC FOLKART SLEIGH BELLS  ZINC METAL HEART DECORATION  ZINC T-LIGHT HOLDER STAR LARGE  ZINC T-LIGHT HOLDER STARS SMALL  ZINC WILLIE WINKIE  CANDLE STICK  amazon adjust  check\n",
    "0                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "2                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "3                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "4                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "...                              ...                 ...                           ...                       ...                      ...                              ...                            ...  ...                        ...                          ...                             ...                              ...                               ...            ...    ...\n",
    "1325                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1326                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1327                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1328                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1329                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "\n",
    "[1330 rows x 1039 columns]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the column headers for sign items\n",
    "sign_headers = [i for i in onehot.columns if i.lower().find('sign')>=0]\n",
    "\"\"\"\n",
    "In [6]:\n",
    "sign_headers\n",
    "Out[6]:\n",
    "\n",
    "['60 CAKE CASES DOLLY GIRL DESIGN',\n",
    " 'AREA PATROLLED METAL SIGN',\n",
    " 'BAKING SET SPACEBOY DESIGN',\n",
    " 'BATHROOM METAL SIGN',\n",
    " 'BEWARE OF THE CAT METAL SIGN ',\n",
    " 'BIRDS MOBILE VINTAGE DESIGN',\n",
    " 'CERAMIC BOWL WITH LOVE HEART DESIGN',\n",
    " 'CERAMIC CAKE DESIGN SPOTTED MUG',\n",
    " 'CHARLOTTE BAG APPLES DESIGN',\n",
    " 'CHARLOTTE BAG SUKI DESIGN',\n",
    " 'CHILDRENS APRON APPLES DESIGN',\n",
    " 'CHILDRENS APRON SPACEBOY DESIGN',\n",
    " 'COFFEE MUG PEARS  DESIGN',\n",
    " 'COOK WITH WINE METAL SIGN ',\n",
    " 'COTTON APRON PANTRY DESIGN',\n",
    " 'FAIRY CAKE DESIGN UMBRELLA',\n",
    " 'FRENCH BLUE METAL DOOR SIGN 3',\n",
    " 'FRENCH BLUE METAL DOOR SIGN 5',\n",
    " 'FRENCH BLUE METAL DOOR SIGN 6',\n",
    " 'FRENCH BLUE METAL DOOR SIGN 9',\n",
    " 'FRENCH BLUE METAL DOOR SIGN No',\n",
    " 'FRENCH TOILET SIGN BLUE METAL',\n",
    " 'FRENCH WC SIGN BLUE METAL',\n",
    " 'GIN + TONIC DIET METAL SIGN',\n",
    " 'HAND OVER THE CHOCOLATE   SIGN ',\n",
    " 'HAND WARMER BABUSHKA DESIGN',\n",
    " 'HAND WARMER BIRD DESIGN',\n",
    " 'HAND WARMER OWL DESIGN',\n",
    " 'HAND WARMER SCOTTY DOG DESIGN',\n",
    " 'HOME SWEET HOME METAL SIGN ',\n",
    " 'JUMBO BAG DOLLY GIRL DESIGN',\n",
    " 'JUMBO BAG SPACEBOY DESIGN',\n",
    " 'KITTENS DESIGN FLANNEL',\n",
    " 'LADIES & GENTLEMEN METAL SIGN',\n",
    " 'LAUNDRY 15C METAL SIGN',\n",
    " 'LUNCH BAG ALPHABET DESIGN',\n",
    " 'LUNCH BAG APPLE DESIGN',\n",
    " 'LUNCH BAG SPACEBOY DESIGN ',\n",
    " 'LUNCH BAG SUKI DESIGN ',\n",
    " 'LUNCH BAG VINTAGE LEAF DESIGN',\n",
    " 'MEMO BOARD COTTAGE DESIGN',\n",
    " 'METAL SIGN DROP YOUR PANTS',\n",
    " 'METAL SIGN EMPIRE TEA',\n",
    " 'METAL SIGN HIS DINNER IS SERVED',\n",
    " 'MONEY BOX KINGS CHOICE DESIGN',\n",
    " 'MONEY BOX POCKET MONEY DESIGN',\n",
    " 'N0 SINGING METAL SIGN',\n",
    " 'NO JUNK MAIL METAL SIGN',\n",
    " 'PARTY METAL SIGN ',\n",
    " 'PEG BAG APPLES DESIGN',\n",
    " 'PLEASE ONE PERSON METAL SIGN',\n",
    " 'POTTERING IN THE SHED METAL SIGN',\n",
    " 'RECIPE BOX PANTRY YELLOW DESIGN',\n",
    " 'RED CHARLIE+LOLA PERSONAL DOORSIGN',\n",
    " 'RIBBON REEL HEARTS DESIGN ',\n",
    " 'RIBBON REEL LACE DESIGN ',\n",
    " 'SET 20 NAPKINS FAIRY CAKES DESIGN ',\n",
    " 'SET OF 3 CAKE TINS PANTRY DESIGN ',\n",
    " 'SET OF 36 DOILIES PANTRY DESIGN',\n",
    " 'SET OF 36 DOILIES SPACEBOY DESIGN ',\n",
    " 'SET OF 6 SPICE TINS PANTRY DESIGN',\n",
    " 'SET OF 60 PANTRY DESIGN CAKE CASES ',\n",
    " 'SMALL DOLLY MIX DESIGN ORANGE BOWL',\n",
    " 'STRIPES DESIGN TEDDY',\n",
    " 'TOILET SIGN OCCUPIED OR VACANT',\n",
    " 'WASHROOM METAL SIGN',\n",
    " 'WORLD WAR 2 GLIDERS ASSTD DESIGNS',\n",
    " 'WRAP ALPHABET DESIGN',\n",
    " 'WRAP POPPIES  DESIGN',\n",
    " \"YOU'RE CONFUSING ME METAL SIGN \"]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Select columns of sign items using sign_headers\n",
    "sign_columns = onehot[sign_headers]\n",
    "\n",
    "\n",
    "# Perform aggregation of sign items into sign category\n",
    "signs = sign_columns.sum(axis = 1) >= 1.0\n",
    "\n",
    "# Print support for signs\n",
    "print('Share of Signs: %.2f' % signs.mean())\n",
    "\n",
    "\"\"\"\n",
    "If you look at the printed statement, you'll notice that support for signs is 0.10, \n",
    "which suggests that signs are an important category of items for the retailer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847dc85",
   "metadata": {},
   "source": [
    "## Defining an aggregation function\n",
    "Surprised by the high share of sign items in its inventory, the retailer decides that it makes sense to do further aggregation for different categories to explore the data better. This seems trivial to you, but the retailer has not previously been able to perform even a basic descriptive analysis of its transaction and items.\n",
    "\n",
    "The retailer asks you to perform aggregation for the candles, bags, and boxes categories. To simplify the task, you decide to write a function. It will take a string that contains an item's category. It will then output a DataFrame that indicates whether each transaction includes items from that category. Note that pandas has been imported for you as pd. Additionally, the data has been imported in one-hot encoded format as onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d88da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(item):\n",
    "\t# Select the column headers for sign items in onehot\n",
    "\titem_headers = [i for i in onehot.columns if i.lower().find(item)>=0]\n",
    "\n",
    "\t# Select columns of sign items\n",
    "\titem_columns = onehot[item_headers]\n",
    "\n",
    "\t# Return category of aggregated items\n",
    "\treturn item_columns.sum(axis = 1) >= 1.0\n",
    "\n",
    "# Aggregate items for the bags, boxes, and candles categories  \n",
    "bags = aggregate('bag')\n",
    "boxes = aggregate('box')\n",
    "candles = aggregate('candle')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "In [1]:\n",
    "bags \n",
    "Out[1]:\n",
    "\n",
    "0       False\n",
    "1        True\n",
    "2       False\n",
    "3       False\n",
    "4        True\n",
    "        ...  \n",
    "1325    False\n",
    "1326    False\n",
    "1327    False\n",
    "1328     True\n",
    "1329    False\n",
    "Length: 1330, dtype: bool\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce41a0",
   "metadata": {},
   "source": [
    "## Pruning and Apriori\n",
    "In the video, we introduced the Apriori algorithm, which made use of the Apriori principle to prune itemsets. The Apriori principle tells us that subsets of frequent itemsets are frequent. Thus, if we find an infrequent itemset, which we'll call {X}, then it must be the case that {X, Y} is also infrequent, so we may eliminate it without computing its support.\n",
    "\n",
    "In this exercise, you'll be given itemsets and information about the frequency of its subsets. You will need to decide whether the information is sufficient to prune the itemset or whether we need to compute its support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import apriori from mlxtend\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# Compute frequent itemsets using the Apriori algorithm\n",
    "frequent_itemsets = apriori(onehot, \n",
    "                            min_support = 0.006, \n",
    "                            max_len = 3, \n",
    "                            use_colnames = True)\n",
    "\n",
    "# Print a preview of the frequent itemsets\n",
    "print(frequent_itemsets.head())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "<script.py> output:\n",
    "        support                              itemsets\n",
    "    0  0.006767          (HOT WATER BOTTLE KEEP CALM)\n",
    "    1  0.007519             (JUMBO BAG RED RETROSPOT)\n",
    "    2  0.006015     (PAPER CHAIN KIT 50'S CHRISTMAS )\n",
    "    3  0.006015                      (POPCORN HOLDER)\n",
    "    4  0.006767  (WHITE HANGING HEART T-LIGHT HOLDER)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46543546",
   "metadata": {},
   "source": [
    "## Selecting a support threshold\n",
    "The manager of the online gift store looks at the results you provided from the previous exercise and commends you for the good work. She does, however, raise an issue: all of the itemsets you identified contain only one item. She asks whether it would be possible to use a less restrictive rule and to generate more itemsets, possibly including those with multiple items.\n",
    "\n",
    "After agreeing to do this, you think about what might explain the lack of itemsets with more than 1 item. It can't be the max_len parameter, since that was set to three. You decide it must be support and decide to test two different values, each time checking how many additional itemsets are generated. Note that pandas is available as pd and the one-hot encoded data is available as onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205cceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import apriori from mlxtend\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "# Compute frequent itemsets using a support of 0.003 and length of 3\n",
    "frequent_itemsets_1 = apriori(onehot, min_support = 0.003, \n",
    "                            max_len = 3, use_colnames = True)\n",
    "\n",
    "# Compute frequent itemsets using a support of 0.001 and length of 3\n",
    "frequent_itemsets_2 = apriori(onehot, min_support = 0.001, \n",
    "                            max_len = 3, use_colnames = True)\n",
    "\n",
    "# Print the number of freqeuent itemsets\n",
    "print(len(frequent_itemsets_1), len(frequent_itemsets_2))\n",
    "\n",
    "\"\"\"\n",
    "91 429\n",
    ">>\n",
    "generated by the Apriori algorithm using a support value of 0.002\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ca849",
   "metadata": {},
   "source": [
    "## Pruning with lift\n",
    "Once again, you report back to the novelty gift store manager. This time, you tell her that you identified no rules when you used a higher support threshold for the Apriori algorithm and only two rules when you used a lower threshold. She commends you for the good work, but asks you to consider using another metric to reduce the two rules to one.\n",
    "\n",
    "You remember that lift had a simple interpretation: values greater than 1 indicate that items co-occur more than we would expect if they were independently distributed across transactions. You decide to use lift, since that message will be simple to convey. Note that pandas is available as pd and the one-hot encoded transaction data is available as onehot. Additionally, apriori has been imported from mlxtend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the association rules function\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Compute frequent itemsets using the Apriori algorithm\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.001, \n",
    "                            max_len = 2, use_colnames = True)\n",
    "\n",
    "# Compute all association rules for frequent_itemsets\n",
    "rules = association_rules(frequent_itemsets, \n",
    "                            metric = \"lift\", \n",
    "                         \tmin_threshold = 1.0)\n",
    "\n",
    "# Print association rules\n",
    "print(rules)\n",
    "\"\"\"\n",
    "<script.py> output:\n",
    "                       antecedents                  consequents  antecedent support  consequent support   support  confidence       lift  leverage  conviction\n",
    "    0    (JUMBO BAG RED RETROSPOT)  (BIRTHDAY CARD, RETRO SPOT)            0.007519            0.002256  0.001504    0.200000  88.666667  0.001487    1.247180\n",
    "    1  (BIRTHDAY CARD, RETRO SPOT)    (JUMBO BAG RED RETROSPOT)            0.002256            0.007519  0.001504    0.666667  88.666667  0.001487    2.977444\n",
    "\n",
    "It looks like you've ended up with two association rules once again, both with lift values greater than 1.0.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ed3a8",
   "metadata": {},
   "source": [
    "## Pruning with confidence\n",
    "Once again, you've come up short: you found multiple useful rules, but can't narrow it down to one. Even worse, the two rules you found used the same itemset, but just swapped the antecedents and consequents. You decide to see whether pruning by another metric might allow you to narrow things down to a single association rule.\n",
    "\n",
    "What would be the right metric? Both lift and support are identical for all rules that can be generated from an itemset, so you decide to use confidence instead, which differs for rules produced from the same itemset. Note that pandas is available as pd and the one-hot encoded transaction data is available as onehot. Additionally, apriori has been imported from mlxtend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f959ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "onehot\n",
    "\n",
    "\n",
    "       50'S CHRISTMAS GIFT BAG LARGE   DOLLY GIRL BEAKER   I LOVE LONDON MINI BACKPACK   RED SPOT GIFT BAG LARGE   SPACEBOY BABY GIFT SET  12 MESSAGE CARDS WITH ENVELOPES  12 PENCIL SMALL TUBE WOODLAND  ...  ZINC FOLKART SLEIGH BELLS  ZINC METAL HEART DECORATION  ZINC T-LIGHT HOLDER STAR LARGE  ZINC T-LIGHT HOLDER STARS SMALL  ZINC WILLIE WINKIE  CANDLE STICK  amazon adjust  check\n",
    "0                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "2                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "3                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "4                              False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "...                              ...                 ...                           ...                       ...                      ...                              ...                            ...  ...                        ...                          ...                             ...                              ...                               ...            ...    ...\n",
    "1325                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1326                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1327                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1328                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "1329                           False               False                         False                     False                    False                            False                          False  ...                      False                        False                           False                            False                             False          False  False\n",
    "\n",
    "[1330 rows x 1039 columns]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde89d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the association rules function\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Compute frequent itemsets using the Apriori algorithm\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.0015, \n",
    "                            max_len = 2, use_colnames = True)\n",
    "\n",
    "# Compute all association rules using confidence\n",
    "rules = association_rules(frequent_itemsets, \n",
    "                            metric = \"confidence\", \n",
    "                            min_threshold = 0.5)\n",
    "\n",
    "# Print association rules\n",
    "print(rules)\n",
    "\"\"\"\n",
    "                   antecedents                consequents  antecedent support  consequent support   support  confidence       lift  leverage  conviction\n",
    "0  (BIRTHDAY CARD, RETRO SPOT)  (JUMBO BAG RED RETROSPOT)            0.002256            0.007519  0.001504    0.666667  88.666667  0.001487    2.977444\n",
    "\n",
    "Notice that we have narrowed things down to just a single rule. We can recommend this to the manager.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a61cd",
   "metadata": {},
   "source": [
    "## Aggregation and filtering\n",
    "In the video, we helped a gift store manager arrange the sections in her physical retail location according to association rules. The layout of the store forced us to group sections into two pairs of product types. After applying advanced filtering techniques, we proposed the floor layout below.\n",
    "\n",
    "The image shows the store layout that was selected in the video.\n",
    "The store manager is now asking you to generate another floorplan proposal, but with a different criterion: each pair of sections should contain one high support product and one low support product. The data, aggregated, has been aggregated and one-hot encoded for you. Additionally, apriori() and association_rules() have been imported from mlxtend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea053444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the apriori algorithm with a minimum support of 0.0001\n",
    "frequent_itemsets = apriori(aggregated, min_support = 0.0001, use_colnames = True)\n",
    "\n",
    "# Generate the initial set of rules using a minimum support of 0.0001\n",
    "rules = association_rules(frequent_itemsets, \n",
    "                          metric = \"support\", min_threshold = 0.0001)\n",
    "\n",
    "# Set minimum antecedent support to 0.35\n",
    "rules = rules[rules['antecedent support'] > 0.35]\n",
    "\n",
    "# Set maximum consequent support to 0.35\n",
    "rules = rules[rules['consequent support'] < 0.35]\n",
    "\n",
    "# Print the remaining rules\n",
    "print(rules)\n",
    "\n",
    "\"\"\"\n",
    "   antecedents     consequents  antecedent support  consequent support   support  confidence      lift  leverage  conviction\n",
    "0        (bag)           (box)            0.466307            0.256065  0.021563    0.046243  0.180590 -0.097841    0.780005\n",
    "2        (bag)        (candle)            0.466307            0.088949  0.010782    0.023121  0.259940 -0.030696    0.932615\n",
    "8       (sign)           (box)            0.355795            0.256065  0.018868    0.053030  0.207097 -0.072239    0.785596\n",
    "10      (sign)        (candle)            0.355795            0.088949  0.008086    0.022727  0.255510 -0.023561    0.932238\n",
    "15      (sign)   (bag, candle)            0.355795            0.010782  0.005391    0.015152  1.405303  0.001555    1.004437\n",
    "16       (bag)  (sign, candle)            0.466307            0.008086  0.005391    0.011561  1.429672  0.001620    1.003515\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "----- Inference ---\n",
    "you'll find both bag -> box and sign -> candles. \n",
    "We can tell the store manager that the original proposal is also acceptable under this new criterion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b5f37",
   "metadata": {},
   "source": [
    "## Applying Zhang's rule\n",
    "In Chapter 2, we learned that Zhang's rule is a continuous measure of association between two items that takes values in the [-1,+1] interval. A -1 value indicates a perfectly negative association and a +1 value indicates a perfectly positive association. In this exercise, you'll determine whether Zhang's rule can be used to refine a set of rules a gift store is currently using to promote products.\n",
    "\n",
    "Note that the frequent itemsets have been computed for you and are available as frequent_itemsets. Additionally, zhangs_rule() has been defined and association_rules() have been imported from mlxtend. You will start by re-computing the original set of rules. After that, you will apply Zhang's metric to select only those rules with a high and positive association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80472a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the initial set of rules using a minimum lift of 1.00\n",
    "rules = association_rules(frequent_itemsets, metric = \"lift\", min_threshold = 1.00)\n",
    "\n",
    "# Set antecedent support to 0.005\n",
    "rules = rules[rules['antecedent support'] > 0.005]\n",
    "\n",
    "# Set consequent support to 0.005\n",
    "rules = rules[rules['consequent support'] > 0.005]\n",
    "\n",
    "# Compute Zhang's rule\n",
    "rules['zhang'] = zhangs_rule(rules)\n",
    "\n",
    "# Set the lower bound for Zhang's rule to 0.98\n",
    "rules = rules[rules['zhang'] > 0.98]\n",
    "print(rules[['antecedents', 'consequents']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc25006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(rules[['antecedents', 'consequents']])\n",
    "                              antecedents                           consequents\n",
    "26                  (BROCADE RING PURSE )      (PANTRY MAGNETIC  SHOPPING LIST)\n",
    "27       (PANTRY MAGNETIC  SHOPPING LIST)                 (BROCADE RING PURSE )\n",
    "84           (HAND WARMER RED LOVE HEART)             (JUMBO BAG PINK POLKADOT)\n",
    "85              (JUMBO BAG PINK POLKADOT)          (HAND WARMER RED LOVE HEART)\n",
    "88           (HAND WARMER RED LOVE HEART)  (WOOD 2 DRAWER CABINET WHITE FINISH)\n",
    "89   (WOOD 2 DRAWER CABINET WHITE FINISH)          (HAND WARMER RED LOVE HEART)\n",
    "148                        (WICKER STAR )                (RED STAR CARD HOLDER)\n",
    "149                (RED STAR CARD HOLDER)                        (WICKER STAR )\n",
    "152      (RIBBON REEL CHRISTMAS PRESENT )  (WOODEN TREE CHRISTMAS SCANDINAVIAN)\n",
    "153  (WOODEN TREE CHRISTMAS SCANDINAVIAN)      (RIBBON REEL CHRISTMAS PRESENT )\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ac0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inferences: Notice that 10 items had a Zhang's metric value of over 0.98,\n",
    "which suggests that the items are nearly perfectly associated in the data. \n",
    "In general, when we see such strong associations,\n",
    "we'll want to think carefully about what explains them. \n",
    "We might, for instance, investigate whether the items be purchased separately or whether they are bundled in a way that prevents this.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8655ab9e",
   "metadata": {},
   "source": [
    "## Advanced filtering with multiple metrics\n",
    "Earlier, we used data from an online novelty gift store to find antecedents that could be used to promote a targeted consequent. Since the set of potential rules was large, we had to rely on the Apriori algorithm and multi-metric filtering to narrow it down. In this exercise, we'll examine the full set of rules and find a useful one, rather than targeting a particular antecedent.\n",
    "\n",
    "Note that the data has been loaded, preprocessed, and one-hot encoded, and is available as onehot. Additionally apriori() and association_rules() have been imported from mlxtend. In this exercise, you'll apply the Apriori algorithm to identify frequent itemsets. You'll then recover the set of association rules from the itemsets and apply multi-metric filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Apriori algorithm with a minimum support threshold of 0.001\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.001, use_colnames = True)\n",
    "\n",
    "# Recover association rules using a minium support threshold of 0.001\n",
    "rules = association_rules(frequent_itemsets, metric = 'support', min_threshold = 0.001)\n",
    "\n",
    "# Apply a 0.002 antecedent support threshold, 0.60 confidence threshold, and 2.50 lift threshold\n",
    "filtered_rules = rules[(rules['antecedent support'] > 0.002) &\n",
    "\t\t\t\t\t\t(rules['consequent support'] > 0.01) &\n",
    "\t\t\t\t\t\t(rules['confidence'] > 0.60) &\n",
    "\t\t\t\t\t\t(rules['lift'] > 2.50)]\n",
    "\n",
    "# Print remaining rule\n",
    "print(filtered_rules[['antecedents','consequents']])\n",
    "\n",
    "\"\"\"\n",
    "                    antecedents                consequents\n",
    "23  (BIRTHDAY CARD, RETRO SPOT)  (JUMBO BAG RED RETROSPOT)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b89d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743dc206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314383e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
